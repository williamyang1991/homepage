<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Controllable Text Style Transfer</title>
<meta name="description" content="Deep Plastic Surgery">
<meta name="author" content="Shuai Yang">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="DPS/css/project.css">
</head>
	<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script> 

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<p class="banner"align="center">Accepted by ECCV 2020</p>
			<h1>Deep Plastic Surgery: Robust and Controllable <br/> Image Editing with Human-Drawn Sketches</h1>
		</div>
		<div class="authors" style="width:95%">
			<div class='author'>
				 <A href="mailto:williamyang@pku.edu.cn" style="text-decoration: none">Shuai Yang</A>
		    </div>
			<div class='author'>
				 <A href="mailto:atlaswang@tamu.edu" style="text-decoration: none">Zhangyang Wang</A>
			</div>		
			<div class='author'>
				 <A href="mailto:liujiaying@pku.edu.cn" style="text-decoration: none">Jiaying Liu</A>
			</div>
			<div class='author'>
				 <A href="mailto:guozongming@pku.edu.cn" style="text-decoration: none">Zongming Guo</A>
			</div>			
		</div>
		<br>
		
		<div class="overview sec">
			<div class="image" style="padding: 2em 0 0.5em 0">
				<table border="0" width='100%' style="FONT-SIZE:15" >
				 <tr align="center">
					<td width="9.70%" align="left"><img src="DPS/figures/teaser-a.png" alt="" width="99%" ></td>
					<td width="13.90%"><img src="DPS/figures/teaser-b.gif" alt="" width="99%" ></td>
					<td width="20.40%"><img src="DPS/figures/teaser-c.png" alt="" width="99%" ></td>
					<td width="9.70%"><img src="DPS/figures/teaser-d.png" alt="" width="99%" ></td>
					<td width="13.90%"><img src="DPS/figures/teaser-e.gif" alt="" width="99%" ></td>	
					<td width="20.40%" align="right"><img src="DPS/figures/teaser-f.png" alt="" width="99%" ></td>
				 </tr>
				 </table>
				 <table border="0" width='100%' style="FONT-SIZE:15" >
				 <tr align="center">
					<td>(a) controllable face synthesis</td><td>(b) controllable face editing</td>
				</tr>
				</table>
				<br/>
				 <table border="0" width='100%' style="FONT-SIZE:15" >
				 <tr align="center">
					<td><img src="DPS/figures/teaser-g.png" alt="" width="99%" ></td>
				</tr>				 	
				 <tr align="center">
					<td>(c) adjusting refinement level <em>l</em></td>
				</tr>
				</table>							 
		  		<p style="text-align: justify">Figure 1. Our Deep Plastic Surgery framework allows users to (a) synthesize and (b) edit photos based on hand-drawn sketches. (c) Our model works robustly on various sketches by setting refinement level <em>l</em> adaptive to the quality of the input sketches, <em>i.e.</em>, higher <em>l</em> for poorer sketches, thus tolerating the drawing errors and achieving the controllability on sketch faithfulness. Note that our model requires no real sketches for training.</p>
	  		</div>
	  	</div>
		
		<div class="abstract_sec">
			<h2>Abstract</h2>
			<div class='desp'>
				<p style="text-align:justify">
					Sketch-based image editing aims to synthesize and modify photos based on the structural information provided by the human-drawn sketches. Since sketches are difficult to collect, previous methods mainly use edge maps instead of sketches to train models (referred to as edge-based models). However, sketches display great structural discrepancy with edge maps, thus failing edge-based models. Moreover, sketches often demonstrate huge variety among different users, demanding even higher generalizability and robustness for the editing model to work. In this paper, we propose Deep Plastic Surgery, a novel, robust and controllable image editing framework that allows users to interactively edit images using hand-drawn sketch inputs. We present a sketch refinement strategy, as inspired by the coarse-to-fine drawing process of the artists, which we show can help our model well adapt to casual and varied sketches without the need for real sketch training data. Our model further provides a refinement level control parameter that enables users to flexibly define how ``reliable'' the input sketch should be considered for the final output, balancing between sketch faithfulness and output verisimilitude (as the two goals might contradict if the input sketch is drawn poorly). To achieve the multi-level refinement, we introduce a style-based module for level conditioning, which allows adaptive feature representations for different levels in a singe network. Extensive experimental results demonstrate the superiority of our approach in improving the visual quality and user controllablity of image editing over the state-of-the-art methods.
				 </p>
			</div>
		</div>

		<div class="abstract_sec">
			<h2>Framework</h2>
			<div class="images">
		  		<img src='DPS/figures/framework.jpg' width='100%' alt="Teaser" >
		  		<p style="text-align: justify">Figure 2. Framework overview. A novel sketch refinement network <em>G</em> is proposed to refine the rough sketch <em>S<sub>l</sub></em> modelled as dilated drawable regions to match the fine edges <em>S<sub>gt</sub></em>. The refined output <em>S<sub>gen</sub></em> is fed into a pretrained edge-based model <em>F</em> to obtain the final editing result <em>I<sub>out</sub></em>. A parameter <em>l</em> is introduced to control the refinement level. It is realized by encoding <em>l</em> into style codes and performing a style-based adjustment over the outputs <em><b>f</b><sub>in</sub></em> of the convolutional layers of <em>G</em> to remove the dilation-based styles.</p>
	  		</div>
	  	</div>	

		
		<div class="download_sec">
			<h2>Resources</h2>
			<div>
				<li><strong>Paper</strong>: <a href="https://arxiv.org/abs/2001.02890">arXiv</a></li>
				<li><strong>Supplementary Material</strong>: <a href="./DPS/files/DeepPlasticSurgery-supp.pdf">PDF</a> (7.53MB)</li>
				<li><strong>Human-Drawn Facial Sketches</strong>: <a href="./DPS/files/human-drawn_facial_sketches.zip">ZIP</a> (11.1MB, our collected 30 facial sketches and results by [3], [4] and our model.)</li>
				<li><strong>Released Code</strong>: <a href="https://github.com/TAMU-VITA/DeepPS">Pytorch implementation</a></li>
			</div>
		</div>

		<div class='citation_sec'>
			<h2>Citation</h2>
			<p class='bibtex'>@inproceedings{Yang2020Deep,
 title={Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches},
 author={Yang, Shuai and Wang, Zhangyang and Liu, Jiaying and Guo, Zongming},
 booktitle={European Conference on Computer Vision},
 year={2020}
}</p>
		</div>

		<div class="experiments_sec">
			<h2>Selected Results</h2>
			<div id="images">
				<img src="DPS/figures/compare1.jpg" alt="" width="100%" >		
				<P style="text-align: justify">Figure 3. Comparison with state-of-the-art methods on face edting. (a) Input photos, masks and sketches. (b) DeepFillv2 [1]. (c) SC-FEGAN [2].  (d) Our results with <em>l</em>=0. (e) Our results with <em>l</em>=1. (f) SC-FEGAN using our refined sketches as input.</P>	
			</div>	
			<div id="images">
				<img src="DPS/figures/compare2.jpg" alt="" width="100%" >		
				<P style="text-align: justify">Figure 4. Comparison with state-of-the-art methods on face synthesis. (a) Input human-drawn sketches. (b) BicycleGAN [3]. (c) pix2pixHD [4].  (d) pix2pix [5]. (e) Our results with <em>l</em>=1. (f) pix2pixHD using our refined sketches as input.</P>	
			</div>			
			
		</div>
		
		<div class="reference_sec">
		<h2>Reference</h2>
		  <div class="bib" style="text-align: justify">
		    <p>[1] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, T.S. Huang. Free-form image inpainting with gated convolution. ICCV 2019.</p>
		    <p>[2] Y. Jo, J. Park. SC-FEGAN: Face editing generative adversarial network with userâ€™s sketch and color. ICCV 2019.</p>
		  	<p>[3] J.Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A.A. Efros, O. Wang, E. Shechtman. Toward multimodal image-to-image translation. NeurIPS 2017.</p>
			<p>[4] T.C. Wang, M.Y. Liu, J.Y. Zhu, A. Tao, J. Kautz, B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CVPR 2018.</p>
			<p>[5] P. Isola, J.Y. Zhu, T. Zhou, A.A. Efros. Image-to-image translation with conditional adversarial networks. CVPR 2017.</p>
		  </div>
	</div>
		
		
		<br></br> 


	<p class="banner"align="center">Last update: July 2020</p>
  </div>
</div>
</body>
</html>
